{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SIn3TrAQdANE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "36dd73c5-43f7-4d3e-cd2a-787f1e83ead3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import torch"
      ],
      "metadata": {
        "id": "dLnQAl6OdFFt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/drive/MyDrive/Colab Notebooks/Qibin/Internship"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "159wgCe3daiV",
        "outputId": "3bba80b8-4aa3-4178-9564-df6dce569c47"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/Colab Notebooks/Qibin/Internship\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd '/content/drive/My Drive/Colab Notebooks/Qibin/Internship/CaloChallenge/code'\n",
        "\n",
        "%run CNN_evaluate.py \\\n",
        "            --input_file ../Datasets/dataset_1_photons_1.hdf5\\\n",
        "            --reference_file ../Datasets/dataset_1_photons_2.hdf5\\\n",
        "            --mode cls-cnn \\\n",
        "            --binning_dataset binning_dataset_1_photons.xml \\\n",
        "            --dataset 1-photons \\\n",
        "            --output_dir eval_metrics_002 \\\n",
        "            --cls_lr 0.00004 \\\n",
        "            --save_mem"
      ],
      "metadata": {
        "id": "eH3V49axqAS7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4da42e6f-ab8d-497d-e5f8-189104347d93"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/My Drive/Colab Notebooks/Qibin/Internship/CaloChallenge/code\n",
            "Checking if input file has the correct form ...\n",
            "Found 121000 events in the file.\n",
            "Checking if input file has the correct form: DONE \n",
            "\n",
            "Extracting showers from input file ...\n",
            "Extracting energy from input file: DONE.\n",
            "\n",
            "Storing reference .pkl file in folder: ../Datasets\n",
            "Checking if reference file has the correct form ...\n",
            "Found 121000 events in the file.\n",
            "Checking if reference file has the correct form: DONE \n",
            "\n",
            "Extracting showers from reference file ...\n",
            "Extracting energy from reference file: DONE.\n",
            "\n",
            "Loading .pkl reference\n",
            "Loading file with high-level features.\n",
            "Calculating high-level features for classifier ...\n",
            "Calculating high-level features for classifer: DONE.\n",
            "\n",
            "Extracting showers from 0.0 file ...\n",
            "Extracting energy from 0.0 file: DONE.\n",
            "\n",
            "Extracting showers from 1.0 file ...\n",
            "Extracting energy from 1.0 file: DONE.\n",
            "\n",
            "Using cuda:0\n",
            "CNN(\n",
            "  (cnn0): Conv2d(1, 32, kernel_size=(3, 1), stride=(1, 1))\n",
            "  (lin0): Sequential(\n",
            "    (0): Linear(in_features=256, out_features=64, bias=True)\n",
            "    (1): LeakyReLU(negative_slope=0.01)\n",
            "    (2): Dropout(p=0.2, inplace=False)\n",
            "  )\n",
            "  (cnn1): Sequential(\n",
            "    (0): Conv2d(1, 32, kernel_size=(3, 4), stride=(1, 1))\n",
            "    (1): MaxPool2d(kernel_size=[2, 2], stride=[2, 2], padding=1, dilation=1, ceil_mode=False)\n",
            "  )\n",
            "  (lin1): Sequential(\n",
            "    (0): Linear(in_features=1440, out_features=64, bias=True)\n",
            "    (1): LeakyReLU(negative_slope=0.01)\n",
            "    (2): Dropout(p=0.2, inplace=False)\n",
            "  )\n",
            "  (cnn2): Sequential(\n",
            "    (0): Conv2d(1, 32, kernel_size=(3, 4), stride=(1, 1))\n",
            "    (1): MaxPool2d(kernel_size=[2, 2], stride=[2, 2], padding=1, dilation=1, ceil_mode=False)\n",
            "  )\n",
            "  (lin2): Sequential(\n",
            "    (0): Linear(in_features=1600, out_features=64, bias=True)\n",
            "    (1): LeakyReLU(negative_slope=0.01)\n",
            "    (2): Dropout(p=0.2, inplace=False)\n",
            "  )\n",
            "  (lin3): Sequential(\n",
            "    (0): Linear(in_features=5, out_features=64, bias=True)\n",
            "    (1): LeakyReLU(negative_slope=0.01)\n",
            "  )\n",
            "  (lin4): Sequential(\n",
            "    (0): Linear(in_features=5, out_features=64, bias=True)\n",
            "    (1): LeakyReLU(negative_slope=0.01)\n",
            "  )\n",
            "  (cnn_final): Sequential(\n",
            "    (0): Conv2d(1, 16, kernel_size=(3, 5), stride=(1, 1), padding=(2, 2))\n",
            "    (1): MaxPool2d(kernel_size=[1, 5], stride=[1, 5], padding=0, dilation=1, ceil_mode=False)\n",
            "    (2): Conv2d(16, 32, kernel_size=(3, 5), stride=(1, 1), padding=(1, 1))\n",
            "    (3): MaxPool2d(kernel_size=[2, 5], stride=[2, 5], padding=0, dilation=1, ceil_mode=False)\n",
            "  )\n",
            "  (lin_final): Sequential(\n",
            "    (0): Linear(in_features=193, out_features=2048, bias=True)\n",
            "    (1): LeakyReLU(negative_slope=0.01)\n",
            "    (2): Dropout(p=0.2, inplace=False)\n",
            "    (3): Linear(in_features=2048, out_features=512, bias=True)\n",
            "    (4): LeakyReLU(negative_slope=0.01)\n",
            "    (5): Dropout(p=0.2, inplace=False)\n",
            "    (6): Linear(in_features=512, out_features=1, bias=True)\n",
            "  )\n",
            ")\n",
            "cls-cnn has 1667745 parameters\n",
            "Epoch   1 / 50, step    0 / 726; loss 0.6934\n",
            "Epoch   1 / 50, step  363 / 726; loss 0.6922\n",
            "Accuracy on training set is 0.5003168044077135\n",
            "Accuracy on test set is 0.4990495867768595\n",
            "AUC on test set is 0.4990495867768595\n",
            "BCE loss of test set is 0.6932, JSD of the two dists is -0.0000\n",
            "Epoch   2 / 50, step    0 / 726; loss 0.6915\n",
            "Epoch   2 / 50, step  363 / 726; loss 0.6938\n",
            "Accuracy on training set is 0.500392561983471\n",
            "Accuracy on test set is 0.5\n",
            "AUC on test set is 0.5\n",
            "BCE loss of test set is 0.6932, JSD of the two dists is -0.0001\n",
            "Epoch   3 / 50, step    0 / 726; loss 0.6926\n",
            "Epoch   3 / 50, step  363 / 726; loss 0.6938\n",
            "Accuracy on training set is 0.5019214876033058\n",
            "Accuracy on test set is 0.5\n",
            "AUC on test set is 0.5\n",
            "BCE loss of test set is 0.6939, JSD of the two dists is -0.0010\n",
            "Epoch   4 / 50, step    0 / 726; loss 0.6907\n",
            "Epoch   4 / 50, step  363 / 726; loss 0.6936\n",
            "Accuracy on training set is 0.4988154269972452\n",
            "Accuracy on test set is 0.5\n",
            "AUC on test set is 0.5\n",
            "BCE loss of test set is 0.6932, JSD of the two dists is -0.0001\n",
            "Epoch   5 / 50, step    0 / 726; loss 0.6929\n",
            "Epoch   5 / 50, step  363 / 726; loss 0.6927\n",
            "Accuracy on training set is 0.4980165289256198\n",
            "Accuracy on test set is 0.5\n",
            "AUC on test set is 0.5\n",
            "BCE loss of test set is 0.6932, JSD of the two dists is -0.0000\n",
            "Epoch   6 / 50, step    0 / 726; loss 0.6950\n",
            "Epoch   6 / 50, step  363 / 726; loss 0.6920\n",
            "Accuracy on training set is 0.49954545454545457\n",
            "Accuracy on test set is 0.5\n",
            "AUC on test set is 0.5\n",
            "BCE loss of test set is 0.6932, JSD of the two dists is -0.0001\n",
            "Epoch   7 / 50, step    0 / 726; loss 0.6936\n",
            "Epoch   7 / 50, step  363 / 726; loss 0.6927\n",
            "Accuracy on training set is 0.5012534435261708\n",
            "Accuracy on test set is 0.5002892561983471\n",
            "AUC on test set is 0.5002892561983471\n",
            "BCE loss of test set is 0.6932, JSD of the two dists is -0.0000\n",
            "Epoch   8 / 50, step    0 / 726; loss 0.6925\n",
            "Epoch   8 / 50, step  363 / 726; loss 0.6944\n",
            "Accuracy on training set is 0.501831955922865\n",
            "Accuracy on test set is 0.5\n",
            "AUC on test set is 0.5\n",
            "BCE loss of test set is 0.6936, JSD of the two dists is -0.0006\n",
            "Epoch   9 / 50, step    0 / 726; loss 0.6946\n",
            "Epoch   9 / 50, step  363 / 726; loss 0.6916\n",
            "Accuracy on training set is 0.4991253443526171\n",
            "Accuracy on test set is 0.5\n",
            "AUC on test set is 0.5\n",
            "BCE loss of test set is 0.6932, JSD of the two dists is -0.0000\n",
            "Epoch  10 / 50, step    0 / 726; loss 0.6940\n",
            "Epoch  10 / 50, step  363 / 726; loss 0.6934\n",
            "Accuracy on training set is 0.5006818181818182\n",
            "Accuracy on test set is 0.5\n",
            "AUC on test set is 0.5\n",
            "BCE loss of test set is 0.6932, JSD of the two dists is -0.0001\n",
            "Epoch  11 / 50, step    0 / 726; loss 0.6915\n",
            "Epoch  11 / 50, step  363 / 726; loss 0.6937\n",
            "Accuracy on training set is 0.5003650137741047\n",
            "Accuracy on test set is 0.49907024793388427\n",
            "AUC on test set is 0.49907024793388427\n",
            "BCE loss of test set is 0.6932, JSD of the two dists is -0.0000\n",
            "Epoch  12 / 50, step    0 / 726; loss 0.6920\n",
            "Epoch  12 / 50, step  363 / 726; loss 0.6930\n",
            "Accuracy on training set is 0.5009641873278237\n",
            "Accuracy on test set is 0.5\n",
            "AUC on test set is 0.5\n",
            "BCE loss of test set is 0.6932, JSD of the two dists is -0.0001\n",
            "Epoch  13 / 50, step    0 / 726; loss 0.6944\n",
            "Epoch  13 / 50, step  363 / 726; loss 0.6934\n",
            "Accuracy on training set is 0.49769283746556475\n",
            "Accuracy on test set is 0.5\n",
            "AUC on test set is 0.5\n",
            "BCE loss of test set is 0.6931, JSD of the two dists is 0.0000\n",
            "Epoch  14 / 50, step    0 / 726; loss 0.6934\n",
            "Epoch  14 / 50, step  363 / 726; loss 0.6929\n",
            "Accuracy on training set is 0.5003236914600551\n",
            "Accuracy on test set is 0.5009297520661157\n",
            "AUC on test set is 0.5009297520661158\n",
            "BCE loss of test set is 0.6932, JSD of the two dists is -0.0000\n",
            "Epoch  15 / 50, step    0 / 726; loss 0.6938\n",
            "Epoch  15 / 50, step  363 / 726; loss 0.6931\n",
            "Accuracy on training set is 0.4988429752066116\n",
            "Accuracy on test set is 0.49907024793388427\n",
            "AUC on test set is 0.49907024793388427\n",
            "BCE loss of test set is 0.6932, JSD of the two dists is -0.0000\n",
            "Epoch  16 / 50, step    0 / 726; loss 0.6928\n",
            "Epoch  16 / 50, step  363 / 726; loss 0.6923\n",
            "Accuracy on training set is 0.5025\n",
            "Accuracy on test set is 0.5\n",
            "AUC on test set is 0.5\n",
            "BCE loss of test set is 0.6933, JSD of the two dists is -0.0002\n",
            "Epoch  17 / 50, step    0 / 726; loss 0.6953\n",
            "Epoch  17 / 50, step  363 / 726; loss 0.6937\n",
            "Accuracy on training set is 0.49944214876033055\n",
            "Accuracy on test set is 0.5\n",
            "AUC on test set is 0.5\n",
            "BCE loss of test set is 0.6932, JSD of the two dists is -0.0000\n",
            "Epoch  18 / 50, step    0 / 726; loss 0.6928\n",
            "Epoch  18 / 50, step  363 / 726; loss 0.6933\n",
            "Accuracy on training set is 0.4974035812672176\n",
            "Accuracy on test set is 0.49892561983471073\n",
            "AUC on test set is 0.4989256198347108\n",
            "BCE loss of test set is 0.6932, JSD of the two dists is -0.0000\n",
            "Epoch  19 / 50, step    0 / 726; loss 0.6931\n",
            "Epoch  19 / 50, step  363 / 726; loss 0.6935\n",
            "Accuracy on training set is 0.4996074380165289\n",
            "Accuracy on test set is 0.49882231404958677\n",
            "AUC on test set is 0.49882231404958677\n",
            "BCE loss of test set is 0.6931, JSD of the two dists is 0.0000\n",
            "Epoch  20 / 50, step    0 / 726; loss 0.6928\n",
            "Epoch  20 / 50, step  363 / 726; loss 0.6934\n",
            "Accuracy on training set is 0.4996694214876033\n",
            "Accuracy on test set is 0.5002892561983471\n",
            "AUC on test set is 0.5002892561983471\n",
            "BCE loss of test set is 0.6932, JSD of the two dists is -0.0000\n",
            "Epoch  21 / 50, step    0 / 726; loss 0.6931\n",
            "Epoch  21 / 50, step  363 / 726; loss 0.6926\n",
            "Accuracy on training set is 0.5002203856749311\n",
            "Accuracy on test set is 0.5\n",
            "AUC on test set is 0.5\n",
            "BCE loss of test set is 0.6932, JSD of the two dists is -0.0000\n",
            "Epoch  22 / 50, step    0 / 726; loss 0.6932\n",
            "Epoch  22 / 50, step  363 / 726; loss 0.6936\n",
            "Accuracy on training set is 0.5006198347107438\n",
            "Accuracy on test set is 0.5\n",
            "AUC on test set is 0.5\n",
            "BCE loss of test set is 0.6931, JSD of the two dists is -0.0000\n",
            "Epoch  23 / 50, step    0 / 726; loss 0.6933\n",
            "Epoch  23 / 50, step  363 / 726; loss 0.6930\n",
            "Accuracy on training set is 0.5009641873278237\n",
            "Accuracy on test set is 0.5\n",
            "AUC on test set is 0.5\n",
            "BCE loss of test set is 0.6932, JSD of the two dists is -0.0001\n",
            "Epoch  24 / 50, step    0 / 726; loss 0.6939\n",
            "Epoch  24 / 50, step  363 / 726; loss 0.6935\n",
            "Accuracy on training set is 0.5012121212121212\n",
            "Accuracy on test set is 0.49882231404958677\n",
            "AUC on test set is 0.49882231404958677\n",
            "BCE loss of test set is 0.6932, JSD of the two dists is -0.0000\n",
            "Epoch  25 / 50, step    0 / 726; loss 0.6928\n",
            "Epoch  25 / 50, step  363 / 726; loss 0.6929\n",
            "Accuracy on training set is 0.5001446280991736\n",
            "Accuracy on test set is 0.5\n",
            "AUC on test set is 0.5\n",
            "BCE loss of test set is 0.6932, JSD of the two dists is -0.0001\n",
            "Epoch  26 / 50, step    0 / 726; loss 0.6940\n",
            "Epoch  26 / 50, step  363 / 726; loss 0.6935\n",
            "Accuracy on training set is 0.4991115702479339\n",
            "Accuracy on test set is 0.5018595041322315\n",
            "AUC on test set is 0.5018595041322313\n",
            "BCE loss of test set is 0.6931, JSD of the two dists is 0.0000\n",
            "Epoch  27 / 50, step    0 / 726; loss 0.6929\n",
            "Epoch  27 / 50, step  363 / 726; loss 0.6935\n",
            "Accuracy on training set is 0.49977272727272726\n",
            "Accuracy on test set is 0.5\n",
            "AUC on test set is 0.5\n",
            "BCE loss of test set is 0.6932, JSD of the two dists is -0.0000\n",
            "Epoch  28 / 50, step    0 / 726; loss 0.6926\n",
            "Epoch  28 / 50, step  363 / 726; loss 0.6934\n",
            "Accuracy on training set is 0.500599173553719\n",
            "Accuracy on test set is 0.5003719008264463\n",
            "AUC on test set is 0.5003719008264463\n",
            "BCE loss of test set is 0.6932, JSD of the two dists is -0.0000\n",
            "Epoch  29 / 50, step    0 / 726; loss 0.6930\n",
            "Epoch  29 / 50, step  363 / 726; loss 0.6929\n",
            "Accuracy on training set is 0.49980716253443525\n",
            "Accuracy on test set is 0.5\n",
            "AUC on test set is 0.5\n",
            "BCE loss of test set is 0.6932, JSD of the two dists is -0.0001\n",
            "Epoch  30 / 50, step    0 / 726; loss 0.6934\n",
            "Epoch  30 / 50, step  363 / 726; loss 0.6932\n",
            "Accuracy on training set is 0.49927685950413225\n",
            "Accuracy on test set is 0.5\n",
            "AUC on test set is 0.5\n",
            "BCE loss of test set is 0.6932, JSD of the two dists is -0.0000\n",
            "Epoch  31 / 50, step    0 / 726; loss 0.6933\n",
            "Epoch  31 / 50, step  363 / 726; loss 0.6935\n",
            "Accuracy on training set is 0.49976584022038567\n",
            "Accuracy on test set is 0.4996280991735537\n",
            "AUC on test set is 0.49962809917355366\n",
            "BCE loss of test set is 0.6932, JSD of the two dists is -0.0000\n",
            "Epoch  32 / 50, step    0 / 726; loss 0.6929\n",
            "Epoch  32 / 50, step  363 / 726; loss 0.6930\n",
            "Accuracy on training set is 0.49860881542699725\n",
            "Accuracy on test set is 0.4996280991735537\n",
            "AUC on test set is 0.49962809917355366\n",
            "BCE loss of test set is 0.6932, JSD of the two dists is -0.0000\n",
            "Epoch  33 / 50, step    0 / 726; loss 0.6930\n",
            "Epoch  33 / 50, step  363 / 726; loss 0.6933\n",
            "Accuracy on training set is 0.4996625344352617\n",
            "Accuracy on test set is 0.5\n",
            "AUC on test set is 0.5\n",
            "BCE loss of test set is 0.6932, JSD of the two dists is -0.0000\n",
            "Epoch  34 / 50, step    0 / 726; loss 0.6933\n",
            "Epoch  34 / 50, step  363 / 726; loss 0.6926\n",
            "Accuracy on training set is 0.499400826446281\n",
            "Accuracy on test set is 0.5\n",
            "AUC on test set is 0.5\n",
            "BCE loss of test set is 0.6932, JSD of the two dists is -0.0000\n",
            "Epoch  35 / 50, step    0 / 726; loss 0.6932\n",
            "Epoch  35 / 50, step  363 / 726; loss 0.6932\n",
            "Accuracy on training set is 0.4992906336088154\n",
            "Accuracy on test set is 0.5018595041322315\n",
            "AUC on test set is 0.5018595041322313\n",
            "BCE loss of test set is 0.6931, JSD of the two dists is -0.0000\n",
            "Epoch  36 / 50, step    0 / 726; loss 0.6928\n",
            "Epoch  36 / 50, step  363 / 726; loss 0.6932\n",
            "Accuracy on training set is 0.497431129476584\n",
            "Accuracy on test set is 0.5025413223140496\n",
            "AUC on test set is 0.5025413223140496\n",
            "BCE loss of test set is 0.6931, JSD of the two dists is -0.0000\n",
            "Epoch  37 / 50, step    0 / 726; loss 0.6929\n",
            "Epoch  37 / 50, step  363 / 726; loss 0.6933\n",
            "Accuracy on training set is 0.500633608815427\n",
            "Accuracy on test set is 0.49907024793388427\n",
            "AUC on test set is 0.49907024793388427\n",
            "BCE loss of test set is 0.6932, JSD of the two dists is -0.0000\n",
            "Epoch  38 / 50, step    0 / 726; loss 0.6930\n",
            "Epoch  38 / 50, step  363 / 726; loss 0.6931\n",
            "Accuracy on training set is 0.49820936639118457\n",
            "Accuracy on test set is 0.49882231404958677\n",
            "AUC on test set is 0.49882231404958677\n",
            "BCE loss of test set is 0.6932, JSD of the two dists is -0.0000\n",
            "Epoch  39 / 50, step    0 / 726; loss 0.6931\n",
            "Epoch  39 / 50, step  363 / 726; loss 0.6933\n",
            "Accuracy on training set is 0.49943526170798896\n",
            "Accuracy on test set is 0.5005785123966943\n",
            "AUC on test set is 0.5005785123966943\n",
            "BCE loss of test set is 0.6932, JSD of the two dists is -0.0000\n",
            "Epoch  40 / 50, step    0 / 726; loss 0.6926\n",
            "Epoch  40 / 50, step  363 / 726; loss 0.6934\n",
            "Accuracy on training set is 0.4993939393939394\n",
            "Accuracy on test set is 0.5004338842975207\n",
            "AUC on test set is 0.5004338842975207\n",
            "BCE loss of test set is 0.6932, JSD of the two dists is -0.0000\n",
            "Epoch  41 / 50, step    0 / 726; loss 0.6939\n",
            "Epoch  41 / 50, step  363 / 726; loss 0.6933\n",
            "Accuracy on training set is 0.49898071625344353\n",
            "Accuracy on test set is 0.4996280991735537\n",
            "AUC on test set is 0.49962809917355366\n",
            "BCE loss of test set is 0.6932, JSD of the two dists is -0.0000\n",
            "Epoch  42 / 50, step    0 / 726; loss 0.6938\n",
            "Epoch  42 / 50, step  363 / 726; loss 0.6933\n",
            "Accuracy on training set is 0.5002134986225896\n",
            "Accuracy on test set is 0.49892561983471073\n",
            "AUC on test set is 0.4989256198347108\n",
            "BCE loss of test set is 0.6932, JSD of the two dists is -0.0000\n",
            "Epoch  43 / 50, step    0 / 726; loss 0.6932\n",
            "Epoch  43 / 50, step  363 / 726; loss 0.6934\n",
            "Accuracy on training set is 0.5001101928374656\n",
            "Accuracy on test set is 0.5\n",
            "AUC on test set is 0.5\n",
            "BCE loss of test set is 0.6932, JSD of the two dists is -0.0000\n",
            "Epoch  44 / 50, step    0 / 726; loss 0.6932\n",
            "Epoch  44 / 50, step  363 / 726; loss 0.6931\n",
            "Accuracy on training set is 0.4986845730027548\n",
            "Accuracy on test set is 0.5\n",
            "AUC on test set is 0.5\n",
            "BCE loss of test set is 0.6932, JSD of the two dists is -0.0000\n",
            "Epoch  45 / 50, step    0 / 726; loss 0.6929\n",
            "Epoch  45 / 50, step  363 / 726; loss 0.6928\n",
            "Accuracy on training set is 0.4989876033057851\n",
            "Accuracy on test set is 0.49892561983471073\n",
            "AUC on test set is 0.4989256198347108\n",
            "BCE loss of test set is 0.6932, JSD of the two dists is -0.0000\n",
            "Epoch  46 / 50, step    0 / 726; loss 0.6934\n",
            "Epoch  46 / 50, step  363 / 726; loss 0.6930\n",
            "Accuracy on training set is 0.4986914600550964\n",
            "Accuracy on test set is 0.49882231404958677\n",
            "AUC on test set is 0.49882231404958677\n",
            "BCE loss of test set is 0.6932, JSD of the two dists is -0.0000\n",
            "Epoch  47 / 50, step    0 / 726; loss 0.6933\n",
            "Epoch  47 / 50, step  363 / 726; loss 0.6934\n",
            "Accuracy on training set is 0.49858815426997244\n",
            "Accuracy on test set is 0.49958677685950414\n",
            "AUC on test set is 0.4995867768595041\n",
            "BCE loss of test set is 0.6932, JSD of the two dists is -0.0000\n",
            "Epoch  48 / 50, step    0 / 726; loss 0.6935\n",
            "Epoch  48 / 50, step  363 / 726; loss 0.6933\n",
            "Accuracy on training set is 0.499931129476584\n",
            "Accuracy on test set is 0.49907024793388427\n",
            "AUC on test set is 0.49907024793388427\n",
            "BCE loss of test set is 0.6932, JSD of the two dists is -0.0000\n",
            "Epoch  49 / 50, step    0 / 726; loss 0.6930\n",
            "Epoch  49 / 50, step  363 / 726; loss 0.6934\n",
            "Accuracy on training set is 0.5005853994490358\n",
            "Accuracy on test set is 0.5000619834710743\n",
            "AUC on test set is 0.5000619834710743\n",
            "BCE loss of test set is 0.6932, JSD of the two dists is -0.0000\n",
            "Epoch  50 / 50, step    0 / 726; loss 0.6930\n",
            "Epoch  50 / 50, step  363 / 726; loss 0.6930\n",
            "Accuracy on training set is 0.5003994490358127\n",
            "Accuracy on test set is 0.5\n",
            "AUC on test set is 0.5\n",
            "BCE loss of test set is 0.6932, JSD of the two dists is -0.0000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from importlib.machinery import SourceFileLoader\n",
        "module = SourceFileLoader(\"CNN_evaluate\", \"/content/drive/My Drive/Colab Notebooks/Qibin/Internship/CaloChallenge/code/CNN_evaluate.py\")\n",
        "\n",
        "source_file = h5py.File(\"../Datasets/dataset_1_photons_1.hdf5\", 'r')\n",
        "\n",
        "hlf = HLF.HighLevelFeatures(\"photon\", filename=\"binning_dataset_1_photons.xml\")\n",
        "\n",
        "classifier = CNN(hlf)\n",
        "classifier.to('cuda:0')\n",
        "\n",
        "x = torch.rand(2, 369).to('cuda:0')\n",
        "\n",
        "\n",
        "print(classifier(x))\n",
        "\n"
      ],
      "metadata": {
        "id": "BW5IhymiPsz7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 202
        },
        "outputId": "35932ac1-767f-432d-9836-4328642205a0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "'NoneType' object has no attribute 'shape'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-4-78af0e7cea62>\u001b[0m in \u001b[0;36m<cell line: 14>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclassifier\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'shape'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(classifier.layers)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Yf1HiO71p1lN",
        "outputId": "b97587f3-3f93-4412-dffe-dcbbd3354ac7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[Conv2d(1, 32, kernel_size=(3, 1), stride=(1, 1)), Sequential(\n",
            "  (0): Linear(in_features=256, out_features=64, bias=True)\n",
            "  (1): LeakyReLU(negative_slope=0.01)\n",
            "  (2): Dropout(p=0.2, inplace=False)\n",
            ")], [Sequential(\n",
            "  (0): Conv2d(1, 32, kernel_size=(3, 4), stride=(1, 1))\n",
            "  (1): MaxPool2d(kernel_size=[2, 2], stride=[2, 2], padding=1, dilation=1, ceil_mode=False)\n",
            "), Sequential(\n",
            "  (0): Linear(in_features=1440, out_features=64, bias=True)\n",
            "  (1): LeakyReLU(negative_slope=0.01)\n",
            "  (2): Dropout(p=0.2, inplace=False)\n",
            ")], [Sequential(\n",
            "  (0): Conv2d(1, 32, kernel_size=(3, 4), stride=(1, 1))\n",
            "  (1): MaxPool2d(kernel_size=[2, 2], stride=[2, 2], padding=1, dilation=1, ceil_mode=False)\n",
            "), Sequential(\n",
            "  (0): Linear(in_features=1600, out_features=64, bias=True)\n",
            "  (1): LeakyReLU(negative_slope=0.01)\n",
            "  (2): Dropout(p=0.2, inplace=False)\n",
            ")], Sequential(\n",
            "  (0): Linear(in_features=5, out_features=64, bias=True)\n",
            "  (1): LeakyReLU(negative_slope=0.01)\n",
            "), Sequential(\n",
            "  (0): Linear(in_features=5, out_features=64, bias=True)\n",
            "  (1): LeakyReLU(negative_slope=0.01)\n",
            "), [Sequential(\n",
            "  (0): Conv2d(1, 16, kernel_size=(3, 5), stride=(1, 1), padding=(2, 2))\n",
            "  (1): MaxPool2d(kernel_size=[1, 5], stride=[1, 5], padding=0, dilation=1, ceil_mode=False)\n",
            "  (2): Conv2d(16, 32, kernel_size=(3, 5), stride=(1, 1), padding=(1, 1))\n",
            "  (3): MaxPool2d(kernel_size=[2, 5], stride=[2, 5], padding=0, dilation=1, ceil_mode=False)\n",
            "), Sequential(\n",
            "  (0): Linear(in_features=193, out_features=512, bias=True)\n",
            "  (1): LeakyReLU(negative_slope=0.01)\n",
            "  (2): Dropout(p=0.2, inplace=False)\n",
            "  (3): Linear(in_features=512, out_features=1, bias=True)\n",
            ")]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd CaloChallenge/code\n",
        "import XMLHandler\n",
        "from HighLevelFeatures import HighLevelFeatures as HLF\n",
        "%cd ../..\n",
        "\n",
        "dataset_photons_train = 'CaloChallenge/Datasets/dataset_1_photons_1.hdf5'\n",
        "f_photons=h5py.File(dataset_photons_train)\n",
        "print(f_photons['incident_energies'].shape)\n",
        "print(f_photons['showers'].shape)\n",
        "\n",
        "HLF_1_photons = HLF('photon', filename='CaloChallenge/code/binning_dataset_1_photons.xml')\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NYoPY0ZNsbob",
        "outputId": "4bf9d2ea-a973-403f-c825-3763403cd741"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Errno 2] No such file or directory: 'CaloChallenge/code'\n",
            "/content/drive/My Drive/Colab Notebooks/Qibin/Internship/CaloChallenge/code\n",
            "/content/drive/My Drive/Colab Notebooks/Qibin/Internship\n",
            "(121000, 1)\n",
            "(121000, 368)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device('cuda:'+'0')\n",
        "\n",
        "\n",
        "data = f_photons['showers'][0]\n",
        "idx = 1\n",
        "data = torch.tensor(data).to(device)\n",
        "\n",
        "data = data[HLF_1_photons.num_alpha[0]*(len(HLF_1_photons.r_edges[0]) - 1):]\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "layer = torch.reshape(data[:HLF_1_photons.num_alpha[idx]*(len(HLF_1_photons.r_edges[idx]) - 1)], (HLF_1_photons.num_alpha[idx],-1)).T\n",
        "data = data[HLF_1_photons.num_alpha[idx]*(len(HLF_1_photons.r_edges[idx]) - 1):]\n",
        "\n",
        "print(layer.shape)\n",
        "input = pad_circular(layer,1).reshape(1,1,18,12)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aG91x-QpEC26",
        "outputId": "ce166ca7-b1f5-4944-9b2f-be60fe223c6c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([16, 10])\n",
            "tensor(5.8849e-05, device='cuda:0')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd '/content/drive/My Drive/Colab Notebooks/Qibin/Internship/CaloChallenge/code'\n",
        "\n",
        "%run CNN_evaluate.py \\\n",
        "            --input_file ../Datasets/dataset_1_photons_1.hdf5\\\n",
        "            --reference_file ../Datasets/dataset_1_photons_2.hdf5\\\n",
        "            --mode cls-low \\\n",
        "            --binning_dataset binning_dataset_1_photons.xml \\\n",
        "            --dataset 1-photons \\\n",
        "            --output_dir eval_metrics_002"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "QZ6YoSlloE_J",
        "outputId": "6dc5a4ae-cff6-460f-ff23-01d6341504d5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/My Drive/Colab Notebooks/Qibin/Internship/CaloChallenge/code\n",
            "Checking if input file has the correct form ...\n",
            "Found 121000 events in the file.\n",
            "Checking if input file has the correct form: DONE \n",
            "\n",
            "Extracting showers from input file ...\n",
            "Extracting energy from input file: DONE.\n",
            "\n",
            "Storing reference .pkl file in folder: ../Datasets\n",
            "Checking if reference file has the correct form ...\n",
            "Found 121000 events in the file.\n",
            "Checking if reference file has the correct form: DONE \n",
            "\n",
            "Extracting showers from reference file ...\n",
            "Extracting energy from reference file: DONE.\n",
            "\n",
            "Loading .pkl reference\n",
            "Loading file with high-level features.\n",
            "Calculating high-level features for classifier ...\n",
            "Calculating high-level features for classifer: DONE.\n",
            "\n",
            "Extracting showers from 0.0 file ...\n",
            "Extracting energy from 0.0 file: DONE.\n",
            "\n",
            "Extracting showers from 1.0 file ...\n",
            "Extracting energy from 1.0 file: DONE.\n",
            "\n",
            "Using cuda:0\n",
            "DNN(\n",
            "  (inputlayer): Linear(in_features=369, out_features=2048, bias=True)\n",
            "  (outputlayer): Linear(in_features=2048, out_features=1, bias=True)\n",
            "  (layers): Sequential(\n",
            "    (0): Linear(in_features=369, out_features=2048, bias=True)\n",
            "    (1): LeakyReLU(negative_slope=0.01)\n",
            "    (2): Dropout(p=0.2, inplace=False)\n",
            "    (3): Linear(in_features=2048, out_features=2048, bias=True)\n",
            "    (4): LeakyReLU(negative_slope=0.01)\n",
            "    (5): Dropout(p=0.2, inplace=False)\n",
            "    (6): Linear(in_features=2048, out_features=2048, bias=True)\n",
            "    (7): LeakyReLU(negative_slope=0.01)\n",
            "    (8): Dropout(p=0.2, inplace=False)\n",
            "    (9): Linear(in_features=2048, out_features=1, bias=True)\n",
            "  )\n",
            ")\n",
            "cls-low has 9152513 parameters\n",
            "Epoch   1 / 50, step    0 / 726; loss 0.6960\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: 'eval_metrics_002/cls-low_1-photons.pt'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/content/drive/My Drive/Colab Notebooks/Qibin/Internship/CaloChallenge/code/CNN-evaluate.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    785\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcls_n_iters\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    786\u001b[0m             \u001b[0mtrain_and_evaluate_cls\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclassifier\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_dataloader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_dataloader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 787\u001b[0;31m             \u001b[0mclassifier\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_classifier\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclassifier\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    788\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    789\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/drive/My Drive/Colab Notebooks/Qibin/Internship/CaloChallenge/code/CNN-evaluate.py\u001b[0m in \u001b[0;36mload_classifier\u001b[0;34m(constructed_model, parser_args)\u001b[0m\n\u001b[1;32m    411\u001b[0m     \u001b[0;34m\"\"\" loads a saved model \"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    412\u001b[0m     \u001b[0mfilename\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparser_args\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'_'\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mparser_args\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'.pt'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 413\u001b[0;31m     checkpoint = torch.load(os.path.join(parser_args.output_dir, filename),\n\u001b[0m\u001b[1;32m    414\u001b[0m                             map_location=parser_args.device)\n\u001b[1;32m    415\u001b[0m     \u001b[0mconstructed_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_state_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcheckpoint\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'model_state_dict'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/serialization.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(f, map_location, pickle_module, weights_only, mmap, **pickle_load_args)\u001b[0m\n\u001b[1;32m    996\u001b[0m         \u001b[0mpickle_load_args\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'encoding'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'utf-8'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    997\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 998\u001b[0;31m     \u001b[0;32mwith\u001b[0m \u001b[0m_open_file_like\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'rb'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mopened_file\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    999\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0m_is_zipfile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopened_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1000\u001b[0m             \u001b[0;31m# The zipfile reader is going to advance the current file position.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/serialization.py\u001b[0m in \u001b[0;36m_open_file_like\u001b[0;34m(name_or_buffer, mode)\u001b[0m\n\u001b[1;32m    443\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_open_file_like\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    444\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0m_is_path\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname_or_buffer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 445\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_open_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    446\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    447\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;34m'w'\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/serialization.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, name, mode)\u001b[0m\n\u001b[1;32m    424\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0m_open_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_opener\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    425\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 426\u001b[0;31m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    427\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    428\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__exit__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'eval_metrics_002/cls-low_1-photons.pt'"
          ]
        }
      ]
    }
  ]
}